{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor   = tf.placeholder(shape=(None, 28, 28, 1), dtype=\"float32\", name=\"input_tensor\")\n",
    "output_tensor  = tf.placeholder(shape=(None, 10), dtype=\"float32\", name=\"output_tensor\")\n",
    "bn_tensor      = tf.placeholder(tf.bool,shape=None)\n",
    "dropout_tensor = tf.placeholder(shape=(None), dtype=\"float32\", name = \"Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2, 2))(input_tensor)\n",
    "layer_2 = tf.keras.layers.BatchNormalization()(layer_1, training = bn_tensor)\n",
    "layer_3 = tf.keras.layers.ReLU()(layer_1)\n",
    "layer_3 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_3)\n",
    "\n",
    "layer_4 = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2))(layer_3)\n",
    "layer_5 = tf.keras.layers.BatchNormalization()(layer_4, training = bn_tensor)\n",
    "layer_6 = tf.keras.layers.ReLU()(layer_4)\n",
    "layer_6 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_6)\n",
    "\n",
    "\n",
    "layer_7  = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), strides = (2, 2))(layer_6)\n",
    "layer_8  = tf.keras.layers.BatchNormalization()(layer_7,training = bn_tensor)\n",
    "layer_9  = tf.keras.layers.ReLU()(layer_7)\n",
    "layer_9  = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_9)\n",
    "\n",
    "\n",
    "\n",
    "layer_10 = tf.keras.layers.AveragePooling2D(pool_size= 2, strides =1)(layer_9) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_11 = tf.keras.layers.Dense(units= 10)(layer_10)\n",
    "layer_12 = tf.reshape(layer_11,shape = (-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d/BiasAdd:0\", shape=(?, 13, 13, 32), dtype=float32)\n",
      "Tensor(\"batch_normalization/cond/Merge:0\", shape=(?, 13, 13, 32), dtype=float32)\n",
      "Tensor(\"dropout_1/cond/Merge:0\", shape=(?, 13, 13, 32), dtype=float32)\n",
      "Tensor(\"conv2d_1/BiasAdd:0\", shape=(?, 6, 6, 64), dtype=float32)\n",
      "Tensor(\"batch_normalization_1/cond/Merge:0\", shape=(?, 6, 6, 64), dtype=float32)\n",
      "Tensor(\"dropout_1_1/cond/Merge:0\", shape=(?, 6, 6, 64), dtype=float32)\n",
      "Tensor(\"conv2d_2/BiasAdd:0\", shape=(?, 2, 2, 128), dtype=float32)\n",
      "Tensor(\"batch_normalization_2/cond/Merge:0\", shape=(?, 2, 2, 128), dtype=float32)\n",
      "Tensor(\"dropout_2/cond/Merge:0\", shape=(?, 2, 2, 128), dtype=float32)\n",
      "Tensor(\"average_pooling2d/AvgPool:0\", shape=(?, 1, 1, 128), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 1, 1, 10), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(layer_1)\n",
    "print(layer_2)\n",
    "print(layer_3)\n",
    "print(layer_4)\n",
    "print(layer_5)\n",
    "print(layer_6)\n",
    "print(layer_7)\n",
    "print(layer_8)\n",
    "print(layer_9)\n",
    "print(layer_10)\n",
    "print(layer_11)\n",
    "print(layer_12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = layer_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=output_tensor, logits=prediction) # tf.reduce_mean(tf.squared_difference(output_tensor, prediction))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(prediction, 1), tf.arg_max(output_tensor, 1)), \"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    loss_optimization = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Train Images :  35000\n",
      "No of Test Images :  7000\n"
     ]
    }
   ],
   "source": [
    "images_list = []\n",
    "for root, folder, files in os.walk(\"/home/grace/Projects/Mnist/Train_Data/\"):\n",
    "    for file in files :\n",
    "        images_list.append(root + file)\n",
    "images_list , test = images_list[0:35000] , images_list[35000:]\n",
    "print(\"No of Train Images : \", len(images_list))\n",
    "print(\"No of Test Images : \", len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch = 10\n",
    "total_images = len(images_list)\n",
    "test_images = len(test)\n",
    "for epoch in range(num_epochs):\n",
    "    shuffle(images_list)\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "    for i in range(0, total_images-batch, batch):\n",
    "        image_batch = []\n",
    "        label_batch = []\n",
    "        for image in images_list[i:i+batch]:\n",
    "            single_image = cv2.imread(image, 0)/255.0 - 0.5\n",
    "            single_image = np.reshape(single_image, (28,28,1))            \n",
    "            image_batch.append(single_image)\n",
    "            label = int(image.split(\"/\")[-1].split(\"_\")[0])\n",
    "            label_batch.append(np.eye(10)[label])\n",
    "        label_batch = np.array(label_batch, dtype=np.float32)\n",
    "        if epoch == 0:            \n",
    "            acc, loss_val = sess.run([accuracy, loss], \\\n",
    "                                     feed_dict={input_tensor :image_batch, output_tensor : label_batch, \\\n",
    "                                               bn_tensor : False, dropout_tensor : 0.0 })\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss_val)\n",
    "        else : \n",
    "            opt, acc, loss_val = sess.run([loss_optimization, accuracy, loss], \\\n",
    "                                          feed_dict={input_tensor :image_batch, output_tensor : label_batch, \\\n",
    "                                                    bn_tensor : True, dropout_tensor : 0.2 })\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss_val)\n",
    "    \n",
    "    shuffle(test)\n",
    "    for j in range(0, test_images-batch, batch):\n",
    "        test_image_batch = []\n",
    "        test_label_batch = []\n",
    "        for test_image in test[j:j+batch]:\n",
    "            single_test_image = cv2.imread(test_image, 0)/255.0 - 0.5\n",
    "            single_test_image = np.reshape(single_test_image, (28,28,1))\n",
    "            test_image_batch.append(single_test_image)\n",
    "            test_label = int(test_image.split(\"/\")[-1].split(\"_\")[0])\n",
    "            test_label_batch.append(np.eye(10)[test_label])\n",
    "        test_label_batch = np.array(test_label_batch, dtype=np.float32)\n",
    "        \n",
    "        test_acc = sess.run([accuracy], \\\n",
    "                            feed_dict={input_tensor :test_image_batch, output_tensor : test_label_batch, \\\n",
    "                                      bn_tensor : True, dropout_tensor : 0.0 })\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "    print(\"Epoch : \" + str(epoch) + \" Loss \" +  str(round(np.average(loss_list), 3)) + \\\n",
    "          \"  Train Accuracy : \" + str(round(np.average(acc_list), 3)) + \"  Test Accuracy : \" + \\\n",
    "          str(round(np.average(test_acc_list), 3)))\n",
    "    if epoch % 10 == 0:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess,'./MNIST_Model/MNIST_TF_MODEL_'+str(epoch))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    [\"Reshape\"])\n",
    "\n",
    "with open('./MNIST_Model/./MNIST_TF_FrozenModel.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
