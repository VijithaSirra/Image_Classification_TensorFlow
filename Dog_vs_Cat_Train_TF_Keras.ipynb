{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor   = tf.placeholder(shape=(None, 64, 64, 3), dtype=\"float32\", name=\"input_tensor\")\n",
    "output_tensor  = tf.placeholder(shape=(None, 2), dtype=\"float32\", name=\"output_tensor\")\n",
    "bn_tensor      = tf.placeholder(tf.bool,shape=None)\n",
    "dropout_tensor = tf.placeholder(shape=(None), dtype=\"float32\", name = \"Dropout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_1 = tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), strides = (2, 2))(input_tensor)\n",
    "layer_2 = tf.keras.layers.BatchNormalization()(layer_1, training = bn_tensor)\n",
    "layer_3 = tf.keras.layers.ReLU()(layer_2)\n",
    "layer_3 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_3)\n",
    "layer_4 = tf.keras.layers.Conv2D(filters = 32, kernel_size = (1,1), strides = (1,1))(layer_3)\n",
    "layer_5 = tf.keras.layers.BatchNormalization()(layer_4, training = bn_tensor)\n",
    "layer_6 = tf.keras.layers.ReLU()(layer_5)\n",
    "layer_6 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_6)\n",
    "\n",
    "\n",
    "layer_7  = tf.keras.layers.Conv2D(filters = 64, kernel_size = (3,3), strides = (2, 2))(layer_6)\n",
    "layer_8  = tf.keras.layers.BatchNormalization()(layer_7,training = bn_tensor)\n",
    "layer_9  = tf.keras.layers.ReLU()(layer_8)\n",
    "layer_9  = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_9)\n",
    "layer_10 = tf.keras.layers.Conv2D(filters = 64, kernel_size = (1,1), strides = (1,1))(layer_9)\n",
    "layer_11 = tf.keras.layers.BatchNormalization()(layer_10, training = bn_tensor)\n",
    "layer_12 = tf.keras.layers.ReLU()(layer_11)\n",
    "layer_12 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_12)\n",
    "\n",
    "\n",
    "layer_13 = tf.keras.layers.Conv2D(filters = 128, kernel_size = (3,3), strides = (1, 1))(layer_12)\n",
    "layer_14 = tf.keras.layers.BatchNormalization()(layer_13, training = bn_tensor)\n",
    "layer_15 = tf.keras.layers.ReLU()(layer_14)\n",
    "layer_15 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_15)\n",
    "layer_16 = tf.keras.layers.Conv2D(filters = 128, kernel_size = (1,1), strides = (1,1))(layer_15)\n",
    "layer_17 = tf.keras.layers.BatchNormalization()(layer_16, training = bn_tensor)\n",
    "layer_18 = tf.keras.layers.ReLU()(layer_17)\n",
    "layer_18 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_18)\n",
    "\n",
    "\n",
    "layer_19 = tf.keras.layers.Conv2D(filters = 256, kernel_size = (3,3), strides = (2, 2))(layer_18)\n",
    "layer_20 = tf.keras.layers.BatchNormalization()(layer_19, training = bn_tensor)\n",
    "layer_21 = tf.keras.layers.ReLU()(layer_20)\n",
    "layer_21 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_21)\n",
    "layer_22 = tf.keras.layers.Conv2D(filters = 256, kernel_size = (1,1), strides = (1,1))(layer_21)\n",
    "layer_23 = tf.keras.layers.BatchNormalization()(layer_22, training = bn_tensor)\n",
    "layer_24 = tf.keras.layers.ReLU()(layer_23)\n",
    "layer_24 = tf.keras.layers.Dropout(rate = dropout_tensor)(layer_24)\n",
    "\n",
    "layer_25 = tf.keras.layers.AveragePooling2D(pool_size= 6, strides =1)(layer_24) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_26 = tf.keras.layers.Dense(units= 2)(layer_25)\n",
    "layer_27 = tf.reshape(layer_26,shape = (-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_1)\n",
    "print(layer_2)\n",
    "print(layer_3)\n",
    "print(layer_4)\n",
    "print(layer_5)\n",
    "print(layer_6)\n",
    "print(layer_7)\n",
    "print(layer_8)\n",
    "print(layer_9)\n",
    "print(layer_10)\n",
    "print(layer_11)\n",
    "print(layer_12)\n",
    "print(layer_13)\n",
    "print(layer_14)\n",
    "print(layer_15)\n",
    "print(layer_16)\n",
    "print(layer_17)\n",
    "print(layer_18)\n",
    "print(layer_19)\n",
    "print(layer_20)\n",
    "print(layer_21)\n",
    "print(layer_22)\n",
    "print(layer_23)\n",
    "print(layer_24)\n",
    "print(layer_25)\n",
    "print(layer_26)\n",
    "print(layer_27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = layer_27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=output_tensor, logits=prediction) # tf.reduce_mean(tf.squared_difference(output_tensor, prediction))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(prediction, 1), tf.arg_max(output_tensor, 1)), \"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    loss_optimization = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = []\n",
    "for root, folder, files in os.walk(\"/home/grace/Projects/Dog_Vs_Cat/Train_data/\"):\n",
    "    for file in files :\n",
    "        images_list.append(root + file)\n",
    "images_list , test = images_list[0:20000] , images_list[20000:]\n",
    "print(\"No of Train Images : \", len(images_list))\n",
    "print(\"No of Test Images  : \", len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess= tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch = 10\n",
    "total_images = len(images_list)\n",
    "test_images = len(test)\n",
    "for epoch in range(num_epochs):\n",
    "    t1 = time.time()\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    test_acc_list = []\n",
    "    for i in range(0, total_images-batch, batch):\n",
    "        image_batch = []\n",
    "        label_batch = []\n",
    "        for image in images_list[i:i+batch]:\n",
    "            single_image = cv2.imread(image)/255.0 - 0.5\n",
    "            single_image = cv2.resize(single_image, (64, 64))\n",
    "            image_batch.append(single_image)\n",
    "            label = int(image.split(\"/\")[-1].split(\"_\")[0])\n",
    "            label_batch.append(np.eye(2)[label])\n",
    "        label_batch = np.array(label_batch, dtype=np.float32)\n",
    "        \n",
    "        if epoch == 0:            \n",
    "            acc, loss_val = sess.run([accuracy, loss], \\\n",
    "                                     feed_dict={input_tensor :image_batch, output_tensor : label_batch, \\\n",
    "                                               bn_tensor : False, dropout_tensor : 0.0 })\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss_val)\n",
    "        else : \n",
    "            opt, acc, loss_val = sess.run([loss_optimization, accuracy, loss], \\\n",
    "                                          feed_dict={input_tensor :image_batch, output_tensor : label_batch, \\\n",
    "                                                    bn_tensor : True, dropout_tensor : 0.2 })\n",
    "            acc_list.append(acc)\n",
    "            loss_list.append(loss_val)\n",
    "    for j in range(0, test_images-batch, batch):\n",
    "        test_image_batch = []\n",
    "        test_label_batch = []\n",
    "        for test_image in test[j:j+batch]:\n",
    "            single_test_image = cv2.imread(test_image)/255.0 - 0.5\n",
    "            single_test_image = cv2.resize(single_test_image, (64, 64))\n",
    "            test_image_batch.append(single_test_image)\n",
    "            test_label = int(test_image.split(\"/\")[-1].split(\"_\")[0])\n",
    "            test_label_batch.append(np.eye(2)[test_label])\n",
    "        test_label_batch = np.array(test_label_batch, dtype=np.float32)\n",
    "        \n",
    "        test_acc = sess.run([accuracy], \\\n",
    "                            feed_dict={input_tensor :test_image_batch, output_tensor : test_label_batch, \\\n",
    "                                      bn_tensor : False, dropout_tensor : 0.0 })\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "    print(\"Epoch : \" + str(epoch) + \" Loss \" +  str(round(np.average(loss_list), 3)) + \\\n",
    "          \"  Train Accuracy : \" + str(round(np.average(acc_list), 3)) + \"  Test Accuracy : \" + \\\n",
    "          str(round(np.average(test_acc_list), 3)))\n",
    "    print(\"Time Takes For This Epoch is : \", time.time() - t1)\n",
    "    print(\"******************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess,'./Dogs_vs_Cats_Model/TF_Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "    sess,\n",
    "    sess.graph_def,\n",
    "    [\"Reshape\"])\n",
    "\n",
    "with open('./Dogs_vs_Cats_Model/TF_FrozenModel.pb', 'wb') as f:\n",
    "    f.write(frozen_graph_def.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
